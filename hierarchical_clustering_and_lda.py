# -*- coding: utf-8 -*-
"""Hierarchical clustering and LDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aTVnAONBsY-fkxMwoHaliANUYLISqmOJ
"""

#import required modules/ libraries
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
import seaborn as sns


#load dataset
total_dataset = pd.read_csv("""replace with file path""", delimiter=',')
total_dataset.tail()

#remove non-numerical values from all columns except the label column and replace with 0s, which is in the final column
##otherwise code will not work
for col in total_dataset.columns[:-1]:
    total_dataset[col] = pd.to_numeric(total_dataset[col], errors="coerce")

total_dataset = total_dataset.replace(np.nan, 0.0, regex=True)

#total_dataset = total_dataset.sort_values(by="Age_mid", ascending=True)
total_dataset.head(5)

total_dataset.columns

#standardise the data
#normalise by using StandardScaler module
"""
Standardising the data makes sure all data is at the same scale, in this approach the data will be scaled to a mean of zero and a standard deviation of 1.
"""

# Assume the last column is the class label
X = total_dataset.iloc[:, 1:-1].values  # Features
y = total_dataset.iloc[:,-1].values   # Class labels

# Encode class labels to numeric values if they are not already numeric
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
class_labels = label_encoder.classes_



# Standardize the data, looking for a mean of 0 and SD of 1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print(X_scaled.shape)
print(np.mean(X_scaled)) #this mean is very close to zero but is not zero exactly due to the limitations of the floating-point arithmetic in Python
print(np.std(X_scaled))
print(X_scaled.shape)

from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering

sil_scores = []
for i in range(2, 11):  # Test cluster sizes from 2 to 10
    model = AgglomerativeClustering(n_clusters=i, linkage='ward')
    cluster_labels = model.fit_predict(X_scaled)
    sil_score = silhouette_score(X_scaled, cluster_labels)
    sil_scores.append(sil_score)

# Plot silhouette scores
plt.plot(range(2, 11), sil_scores)
plt.title('Silhouette Score')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()

# Step 1: Example dataset
df = total_dataset  # Replace with your actual dataset

# Specify the columns to be used in hierarchical clustering (exclude Depth)
selected_columns = ['Asx_DL_F', 'Glx_DL_F', 'Ser_DL_F', 'Ala_DL_F', 'Va_DL_F', 'Phe_DL_F',
       '[Ser]/[Ala]_F', 'Asx_DL_H',
       'Glx_DL_H', 'Ser_DL_H', 'Ala_DL_H', 'Va_DL_H', 'Phe_DL_H',
       '[Ser]/[Ala]_H']
df_selected = df[selected_columns + ['Depth']]  # Retain Depth for grouping but not for clustering

# Step 2: Aggregate the features by category (group by Depth)
category_features = df_selected.groupby('Depth').mean()

# Step 3: Preprocess the data (scale it), excluding Depth
scaler = StandardScaler()
scaled_features = scaler.fit_transform(category_features[selected_columns])  # Scale only selected columns

# Step 4: Perform hierarchical clustering on the scaled category features
linkage_matrix = linkage(scaled_features, method='ward')

# Step 5: Plot the dendrogram to visualize clustering
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix, labels=category_features.index)
plt.title("Hierarchical Clustering Dendrogram for nURG core horizons")
plt.xlabel("Depth/ m")
plt.ylabel("Distance")
plt.show()

# Step 6: Cut the dendrogram to form clusters (let's say we want 7 clusters)
num_clusters = 6
cluster_labels = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')

# Step 7: Map the cluster labels back to the original categories
category_features['Cluster'] = cluster_labels

# Step 8: Now assign the cluster labels to the original dataframe based on Depth
total_dataset['Cluster'] = total_dataset['Depth'].map(category_features['Cluster'])

NewAgeRank_dic = { 1:1, 2:2, 5:3, 4:4, 6:5, 3:6, 7:7}

total_dataset["New_AgeRank"] = total_dataset["Cluster"].map(NewAgeRank_dic)
total_dataset.columns

# Step 9: Perform LDA to confirm the clusters
X = total_dataset.iloc[:, 1:-3]   # Features excluding Depth
scaler = StandardScaler()
features = scaler.fit_transform(X)

labels = total_dataset['New_AgeRank']  # Cluster labels

# Reduce to 2 components for visualization
lda_2d = LDA(n_components=5)
X_lda_2d = lda_2d.fit_transform(features, labels)

X_lda_df = pd.DataFrame(X_lda_2d, columns=['LDA1', 'LDA2', 'LDA3', 'LDA4', 'LDA5'])
X_lda_df['Age'] = labels




palette_ageclassification = {
    1.0: '#0343df', 2.0: '#380282', 3.0: '#ff81c0',
    4.0: '#7e1e9c', 5.0: '#d1b26f', 6.0: '#75bbfd'}


# Step 10: Visualize the LDA-transformed data
plt.figure(figsize=(10, 8))
sns.scatterplot(data=X_lda_df, x='LDA1', y='LDA2', hue='Age', palette=palette_ageclassification, s=100)
plt.title('LDA Confirmation of Clusters')
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.show()

# Loadings of features on each principal component
coefficients = lda_2d.coef_
lda_scalings = lda_2d.scalings_


# Plot the coefficients for each LDA component
plt.subplots(figsize=(40, 6))
for i in range(lda_scalings.shape[1]):
    plt.subplot(1, lda_scalings.shape[1], i + 1)
    plt.barh(features, lda_scalings[:, i])
    plt.title(f'LDA Component {i+1}')
    plt.xlabel('Coefficient Value')
    plt.ylabel('Feature')

plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.8, hspace=0.4)
plt.show()